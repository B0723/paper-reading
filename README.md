# paper-reading

### Guideline:
- paper reading讲解的时候要深入浅出，确保自己看懂了，再用通俗的话讲出来。关键是把文章工作讲清楚，motivation，方法部分，实验是否支撑，该工作的优点和缺点，对你个人工作的启发。最重要的是后面两部分，需要你自己对工作批判性的阅读。
- 分享的同学务必提前告知大家分享的论文，并在分享前update paper信息及slides到 [nuaa-nlp/paper-reading](https://github.com/nuaa-nlp/paper-reading)；新人权限开通请联系pjli。
- 参与者希望都能够提前把分享的paper进行相关背景的了解，积极提出问题及参与讨论。

### Next Meeting

### 2021/10/28

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|ZiHao Deng|CVPR 2021 [MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation](https://openaccess.thecvf.com/content/CVPR2021/papers/Kariyappa_MAZE_Data-Free_Model_Stealing_Attack_Using_Zeroth-Order_Gradient_Estimation_CVPR_2021_paper.pdf)|[[slides]](./slides/20211028_ZiHaoDeng.pptx)|-|

### 2021/10/21

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|Zhicheng Li|unpublish 2021 [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.](https://arxiv.org/pdf/2107.13586.pdf)|[[slides]](./slides/20211021_ZhichengLi.pdf)|-|


### 2021/09/16

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|Yundi Shi|EMNLP 2017 [Adversarial Examples for Evaluating Reading Comprehension Systems.](https://aclanthology.org/D17-1215.pdf)|[[slides]](./slides/20210916_YundiShi.pdf)|-|
|-|ACL 2019 [Improving the Robustness of Question Answering Systems to Question Paraphrasing.](https://aclanthology.org/P19-1610.pdf) |-|-|


### 2021/09/02

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|Xuan Sheng|USENIX 2020 [Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries](https://www.usenix.org/conference/usenixsecurity20/presentation/suya)|[[slides]](./slides/20210902_XuanSheng.pdf)|-|
|-|CCS 2020 [Gotta Catch'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks](https://dl.acm.org/doi/10.1145/3372297.3417231) |-|-|


### 2021/08/18

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|Changchun Yin|ICLR 2014 [Intriguing properties of neural networks](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.674.1080&rep=rep1&type=pdf)|[[slides]](./slides/20210818_ChangchunYin_AE_CV.pdf)|-|
|-|ICLR 2015 [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572) |-|-|
|-|IJCAI 2018 [Generating Adversarial Examples with Adversarial Networks](https://www.ijcai.org/proceedings/2018/0543.pdf) |-|-|
|-|TEC 2019 [One Pixel Attack for Fooling Deep Neural Networks](https://ieeexplore.ieee.org/abstract/document/8601309/) |-|-|


### 2021/07/28

|Speakers|Papers|Slides|Others|
|:----:|:----|:----:|:-----:|
|Zhaoyang Han|USENIX 2020 [TEXTSHIELD: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation](https://www.usenix.org/conference/usenixsecurity20/presentation/li-jinfeng)|[[slides]](./slides/20210728_ZhaoyangHan.pdf)|-|
|-|NDSS 2019 [TextBugger: Generating Adversarial Text Against Real-world Applications](https://arxiv.org/abs/1812.05271) |-|-|
